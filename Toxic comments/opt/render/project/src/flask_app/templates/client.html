<!DOCTYPE html>
<html>
    <head>
        <title>Toxic Comment Detector</title>
        <link
            rel="stylesheet"
            href="https://fonts.googleapis.com/css2?family=Inter:wght@200..700&display=swap"
        />
        <style>
            body {
                font-family: "Inter", Arial, sans-serif;
                max-width: 800px;
                margin: 0 auto;
                padding: 20px;
            }
            h1 {
                font-size: 3em;
            }
            h1,
            h2 {
                text-align: center;
            }
            p {
                font-size: 1em;
                color: #3f3f3f;
            }
            textarea {
                width: -webkit-fill-available;
                height: 150px;
                margin: 15px 0;
                border-radius: 8px;
                border: 2px solid #3f3f3f;
                background: #fefef6;
                font-size: 16px;
                padding: 10px;
            }
            #analyzeBtn {
                padding: 1em 1.1em;
                border: 2px solid #3f3f3f;
                border-radius: 6px;
                background-color: #eaeaea;
                color: #3f3f3f;
                font-weight: 700;
                font-size: 18px;
                cursor: pointer;
                transition: all 0.3s ease;
            }
            #analyzeBtn:hover {
                background-color: #dcdcdc;
            }
            #result {
                margin-top: 3em;
            }
            .toxic {
                background-color: #ffdddd;
                padding: 15px;
                border-left: 3px solid red;
            }
            .non-toxic {
                background-color: #ddffdd;
                padding: 15px;
                border-left: 3px solid green;
            }
            #details {
                display: flex;
                align-items: center;
            }
            .category {
                margin: 5px 0;
                text-align: center;
            }
            .category-name {
                font-weight: 600;
            }
            .category-score {
                margin-left: 10px;
                color: #3f3f3f;
            }
            .collapsible {
                display: none;
            }
            .read-more-btn {
                color: #7f7f7f;
                cursor: pointer;
                text-decoration: underline;
            }
            .read-more-btn:hover {
                color: #636363;
            }
            @media (max-width: 1024px) {
                p {
                    font-size: 1.8em;
                    color: #3f3f3f;
                }
                textarea {
                    margin-top: 2.8em;
                    height: 300px;
                    font-size: 18px;
                }
                textarea::placeholder {
                    font-size: 3rem;
                }
                #analyzeBtn {
                    width: 100%;
                    font-size: 3rem;
                    padding: 1.1em 1.2em;
                }
                #details {
                    display: block;
                    padding-top: 1em;
                }
                .category {
                    text-align: left;
                }
            }
        </style>
    </head>
    <body>
        <h1>Toxic Comment Detector</h1>
        <p>
            This simple AI-powered tool uses advanced Natural Language
            Processing (NLP) and Machine Learning (ML) to automatically identify
            harmful content in online comments.
            <span id="readMoreBtn" class="read-more-btn">Read more...</span>
        </p>
        <div id="moreContent" class="collapsible">
            <p>
                Powered by a custom AI model trained with SpaCy, the system
                analyzes text for multiple toxicity categories by recognizing
                patterns in language that typically indicate harmful intent. The
                model evaluates each comment using its neural network and
                assigns confidence scores to different types of toxicity, based
                on what it learned during training from thousands of sample
                English-language comments.
            </p>
            <p>
                While no AI is perfect, this implementation demonstrates how NLP
                technologies can help moderate online discussions by flagging
                potentially toxic content for human review. Test it yourself!
            </p>
        </div>

        <textarea
            id="commentInput"
            placeholder="Enter a comment to analyze..."
        ></textarea>
        <button id="analyzeBtn">Analyze Comment</button>
        <div id="result"></div>
        <div id="details"></div>

        <script>
            // Read more functionality
            const readMoreBtn = document.getElementById("readMoreBtn");
            const moreContent = document.getElementById("moreContent");

            readMoreBtn.addEventListener("click", function () {
                if (moreContent.classList.contains("collapsible")) {
                    moreContent.classList.remove("collapsible");
                    readMoreBtn.textContent = "Show less";
                } else {
                    moreContent.classList.add("collapsible");
                    readMoreBtn.textContent = "Read more...";
                }
            });

            // Analysis functionality
            document
                .getElementById("analyzeBtn")
                .addEventListener("click", analyzeComment);

            async function analyzeComment() {
                const comment = document.getElementById("commentInput").value;
                if (!comment) {
                    alert("Please enter a comment");
                    return;
                }

                try {
                    const response = await fetch("/", {
                        method: "POST",
                        headers: { "Content-Type": "application/json" },
                        body: JSON.stringify({ text: comment }),
                    });

                    const data = await response.json();

                    if (response.status !== 200) {
                        throw new Error(data.error || "Unknown error");
                    }

                    displayResults(data);
                } catch (error) {
                    document.getElementById(
                        "result"
                    ).innerHTML = `<div class="toxic">Error: ${error.message}</div>`;
                    console.error("Error:", error);
                }
            }

            function displayResults(data) {
                const resultDiv = document.getElementById("result");
                const detailsDiv = document.getElementById("details");

                const isToxic = Object.values(data.predictions).some(
                    (score) => score > 0.5
                );

                resultDiv.innerHTML =
                    "<h2>Analysis Result</h2>" +
                    (isToxic
                        ? `<div class="toxic">⚠️  Toxic content detected (max confidence: ${data.overall_score.toFixed(
                              2
                          )})</div>`
                        : `<div class="non-toxic">✅  No toxic content detected (max confidence: ${data.overall_score.toFixed(
                              2
                          )})</div>`);

                detailsDiv.innerHTML =
                    "<h3>Toxicity Breakdown:</h3>" +
                    Object.entries(data.predictions)
                        .map(
                            ([category, score]) => `
                        <div class="category">
                            <span class="category-name">${category.replace(
                                "_",
                                " "
                            )}:</span>
                            <span class="category-score">${score.toFixed(
                                2
                            )}</span>
                        </div>
                    `
                        )
                        .join("");
            }
        </script>
    </body>
</html>
