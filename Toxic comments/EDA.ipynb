{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train.csv\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Info\n",
    "print(\"Dataset Info:\")\n",
    "print(train_data.info())\n",
    "print(\"\\nSample Data:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Distribution\n",
    "label_cols = [\"toxic\", \"insult\", \"obscene\", \"threat\", \"identity_hate\"]  # Update with your label columns\n",
    "label_counts = train_data[label_cols].sum()\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_eda(text):\n",
    "    \"\"\"\n",
    "    A simple preprocessing function for EDA.\n",
    "    Retains as much context as possible while cleaning noise.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase to make analysis case-insensitive\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs with a placeholder\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Replace user mentions (e.g., @username) with a placeholder\n",
    "    text = re.sub(r\"@\\w+\", \"<USER>\", text)\n",
    "\n",
    "    # Replace hashtags (#topic) with the word itself\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data for EDA\n",
    "train_data['cleaned_comment'] = train_data['comment_text'].apply(preprocess_text_eda)\n",
    "print(train_data[['comment_text', 'cleaned_comment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original vs. cleaned comment lengths\n",
    "train_data['original_length'] = train_data['comment_text'].apply(len)\n",
    "train_data['cleaned_length'] = train_data['cleaned_comment'].apply(len)\n",
    "\n",
    "print(train_data[['original_length', 'cleaned_length']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(train_data.info())\n",
    "\n",
    "# Distribution of comment lengths\n",
    "train_data['cleaned_length'] = train_data['cleaned_comment'].apply(len)\n",
    "train_data['word_count'] = train_data['cleaned_comment'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nSummary of cleaned comment lengths:\")\n",
    "print(train_data['cleaned_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of comment lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train_data['cleaned_length'], bins=50, kde=True, color='blue')\n",
    "plt.title(\"Distribution of Cleaned Comment Lengths\")\n",
    "plt.xlabel(\"Comment Length (characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution of word counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train_data['word_count'], bins=50, kde=True, color='green')\n",
    "plt.title(\"Distribution of Word Counts in Cleaned Comments\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_negative_comments(text):\n",
    "    \"\"\"\n",
    "    Additional preprocessing for negative comments wordcloud\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespaces\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)  # Normalize repeated characters (\"soooo\" -> \"so\")\n",
    "    text = re.sub(r\"\\bn+ig+e*r+\\b\", \"nigger\", text)  # Normalize variations of offensive terms\n",
    "    text = re.sub(r\"\\bf+u+c*k+\\b\", \"fuck\", text)  # Standardize curse words\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove remaining punctuations\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    text = ' '.join(filtered_words)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Define negative comments\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_data[\"negative_comment\"] = train_data[label_columns].max(axis=1)\n",
    "\n",
    "# Split into negative and non-negative groups\n",
    "negative_comments = train_data[train_data[\"negative_comment\"] == 1].copy()\n",
    "non_negative_comments = train_data[train_data[\"negative_comment\"] == 0].copy()\n",
    "\n",
    "# Explicitly create a copy of the subset to avoid the SettingWithCopyWarning\n",
    "negative_comments = negative_comments.copy()\n",
    "\n",
    "# Apply preprocessing to comments\n",
    "negative_comments[\"cleaned_comment\"] = negative_comments[\"cleaned_comment\"].apply(preprocess_negative_comments)\n",
    "non_negative_comments[\"cleaned_comment\"] = non_negative_comments[\"cleaned_comment\"].apply(preprocess_negative_comments)\n",
    "\n",
    "# Combine all cleaned comments into a single string for each category\n",
    "negative_text = \" \".join(negative_comments[\"cleaned_comment\"].dropna())\n",
    "non_negative_text = \" \".join(non_negative_comments[\"cleaned_comment\"].dropna())\n",
    "\n",
    "# Count word frequencies\n",
    "negative_word_counts = Counter(negative_text.split())\n",
    "non_negative_word_counts = Counter(non_negative_text.split())\n",
    "\n",
    "# Debug: Inspect word frequencies\n",
    "print(\"Top 20 words in negative comments:\", negative_word_counts.most_common(20))\n",
    "print(\"Top 20 words in non-negative comments:\", non_negative_word_counts.most_common(20))\n",
    "\n",
    "# Generate word clouds from frequencies\n",
    "wordcloud_negative = WordCloud(\n",
    "    width=800, height=400, background_color=\"black\", colormap=\"Reds\"\n",
    ").generate_from_frequencies(negative_word_counts)\n",
    "\n",
    "wordcloud_non_negative = WordCloud(\n",
    "    width=800, height=400, background_color=\"black\", colormap=\"Greens\"\n",
    ").generate_from_frequencies(non_negative_word_counts)\n",
    "\n",
    "# Plot Word Clouds\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Negative comments word cloud\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud_negative, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words in Negative Comments\", fontsize=16)\n",
    "\n",
    "# Non-negative comments word cloud\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(wordcloud_non_negative, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Most Frequent Words in Non-Negative Comments\", fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "label_distribution = train_data[label_cols].sum().sort_values(ascending=False)\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_distribution)\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=label_distribution.index, y=label_distribution.values, palette=\"viridis\")\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Labels\")\n",
    "plt.show()\n",
    "\n",
    "# Relationship between labels and comment lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train_data, x='negative_comment', y='cleaned_length', palette=\"coolwarm\")\n",
    "plt.title(\"Comment Length by Negativity\")\n",
    "plt.xlabel(\"Negativity (0 = Non-Negative, 1 = Negative)\")\n",
    "plt.ylabel(\"Comment Length (characters)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations\n",
    "label_corr = train_data[label_cols].corr()\n",
    "\n",
    "# Heatmap for label correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(label_corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Labels\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extremely short comments\n",
    "short_comments = train_data[train_data['cleaned_length'] < 10]\n",
    "print(\"\\nExamples of very short comments:\")\n",
    "print(short_comments[['cleaned_comment', 'toxic', 'obscene', 'insult']].head())\n",
    "\n",
    "# Extremely long comments\n",
    "long_comments = train_data[train_data['cleaned_length'] > 500]\n",
    "print(\"\\nExamples of very long comments:\")\n",
    "print(long_comments[['cleaned_comment', 'toxic', 'obscene', 'insult']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select label columns\n",
    "label_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# Compute co-occurrence matrix\n",
    "co_occurrence = train_data[label_columns].T.dot(train_data[label_columns])\n",
    "\n",
    "# Normalize the matrix to get proportions (optional)\n",
    "co_occurrence_normalized = co_occurrence / len(train_data)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_occurrence, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Label Co-Occurrence Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique combinations of labels\n",
    "train_data[\"label_combination\"] = train_data[label_columns].apply(lambda row: tuple(row), axis=1)\n",
    "combination_counts = train_data[\"label_combination\"].value_counts()\n",
    "\n",
    "# Display the top 10 most frequent combinations\n",
    "print(\"Top 10 most common label combinations:\")\n",
    "print(combination_counts.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
